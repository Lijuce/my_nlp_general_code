{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit ('EmbedKGQA': conda)",
   "metadata": {
    "interpreter": {
     "hash": "12c76d2922aba7a8deb222fd062ad79b6c97e8ea193c751a7c81a3c42048549f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import AlbertConfig\n",
    "from transformers import AlbertForMaskedLM\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import tensorboard\n"
   ]
  },
  {
   "source": [
    "## 分词器训练"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = \"./text.txt\" # 训练文本文件\n",
    "vocab_size = 20000\n",
    "min_frequency = 1\n",
    "limit_alphabet = 20000\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] #适用于Bert和Albert\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=True,\n",
    ")\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(\n",
    "    files,\n",
    "    vocab_size = vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    show_progress=True,\n",
    "    special_tokens=special_tokens,\n",
    "    limit_alphabet=limit_alphabet,\n",
    "    wordpieces_prefix=\"##\"\n",
    "    )\n",
    "    \n",
    "# !mkdir tokenizer\n",
    "tokenizer.save_model(\"tokenizer\")  # to vocab.txt\n",
    "tokenizer.save(\"./tokenizer/tokenizer\")"
   ]
  },
  {
   "source": [
    "## 模型配置"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13345312"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "config = AlbertConfig(\n",
    "    vocab_size = 20000,\n",
    "    embedding_size = 256,\n",
    "    hidden_size = 768,\n",
    "    num_hidden_layers = 6,\n",
    "    num_attention_heads = 12,\n",
    "    intermediate_size = 3072,\n",
    "    hidden_act = \"gelu\",\n",
    "    hidden_dropout_prob = 0.1,\n",
    "    attention_probs_dropout_prob = 0.1,\n",
    "    max_position_embeddings = 512,\n",
    "    type_vocab_size = 2,\n",
    "    initializer_range = 0.02,\n",
    "    layer_norm_eps = 1e-12,\n",
    ")\n",
    "\n",
    "model = AlbertForMaskedLM(config=config)\n",
    "model.num_parameters()  # 查看参数量"
   ]
  },
  {
   "source": [
    "## 导入训练好的分词器"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 12.8 s, sys: 40.8 s, total: 53.6 s\nWall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"./tokenizer\", max_len=100)\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./text.txt\",\n",
    "    block_size=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./textAlbert\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_gpu_train_batch_size=16,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, description='Epoch', max=2, style=ProgressStyle(description_width='initial…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7c9ee4b19884c518ca6922923ad43f0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, description='Iteration', max=6250, style=ProgressStyle(description_width='…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c0a31e0847f4fcc8cdba2c977192e58"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, description='Iteration', max=6250, style=ProgressStyle(description_width='…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6aa186e7f87b40b58fff2abf108dafbd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\nCPU times: user 18min 48s, sys: 53.1 s, total: 19min 41s\nWall time: 13min 32s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=5.10019878900528)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "%%time\n",
    "# 启动训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}